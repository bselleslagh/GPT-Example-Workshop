{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser, os\n",
    "import numpy as np\n",
    "import openai\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import tiktoken\n",
    "\n",
    "# Set the API key and load the configuration file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../config.ini')\n",
    "openai.api_key = config['openai']['api_key']\n",
    "os.environ['OPENAI_API_KEY'] = config['openai']['api_key']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fine-Tuning\n",
    "In this notebook we will retrain the model by fine-tuning GPT-3 with an example dataset.\n",
    "We can then ask questions to the model related to the dataset we just fine-tuned it with.\n",
    "\n",
    "![Model Selection](../images/fine_tuning_model_selection.png)\n",
    "\n",
    "\n",
    "*Illustrative examples of text classification performance on the Stanford Natural Language Inference (SNLI) Corpus, in which ordered pairs of sentences are classified by their logical relationship: either contradicted, entailed (implied), or neutral. Default fine-tuning parameters were used when not otherwise specified.*\n",
    "\n",
    "For complex tasks, requiring subtle interpretation or reasoning or prior knowledge or coding ability, the performance gaps between models can be larger, and better models like curie or text-davinci-002 could be the best fit.\n",
    "\n",
    "**A single project might end up trying all models. One illustrative development path might look like this:**\n",
    "- Test code using the cheapest & fastest model (ada)\n",
    "- Run a few early experiments to check whether your dataset works as expected with a middling model (curie)\n",
    "- Run a few more experiments with the best model to see how far you can push performance (text-davinci-002)\n",
    "- Once you have good results, do a training run with all models to map out the price-performance frontier and select the model that makes the most sense for your use case  (ada, babbage, curie, davinci, text-davinci-002)\n",
    "\n",
    "**Another possible development path that uses multiple models could be:**\n",
    "- Starting with a small dataset, train the best possible model (text-davinci-002)\n",
    "- Use this fine-tuned model to generate many more labels and expand your dataset by multiples\n",
    "- Use this new dataset to train a cheaper model (ada)\n",
    "\n",
    "\n",
    "More info about model fine-tuning can be found [here](https://docs.google.com/document/d/1rqj7dkuvl7Byd5KQPUJRxc19BJt8wo0yHNwK84KfU3Q/edit#).\n",
    "\n",
    "In this notebook we will fine-tune the model with FAQ data coming from https://coolblue.be. We will then ask questions to the model related to the dataset we just fine-tuned it with.\n",
    "\n",
    "## Step 1: Preprocess the FAQ data\n",
    "The preprocessing step is crucial for ensuring that the FAQ data is in a format that can be easily processed by the model. Here are some common preprocessing steps for text data:\n",
    "\n",
    "1. **Lowercase the text**: Converting all text to lowercase can help reduce the size of the vocabulary and make the model more robust to variations in capitalization.\n",
    "\n",
    "2. **Tokenize the text**: Tokenization involves splitting the text into individual tokens (e.g., words or subwords) that can be fed into the model. This can be done using a tokenizer, such as the one provided by the Hugging Face Transformers library.\n",
    "\n",
    "3. **Remove punctuation and special characters**: Removing punctuation and special characters can help reduce the size of the vocabulary and make the model more robust to variations in the input.\n",
    "\n",
    "4. **Remove stop words**: Stop words are common words that are unlikely to carry much meaning, such as \"a\", \"an\", \"the\", etc. Removing stop words can reduce the size of the vocabulary and improve the efficiency of the model.\n",
    "\n",
    "5. **Convert words to IDs**: The model operates on numbers, not words, so we need to convert each word in the text to a unique integer ID. This can be done using a vocabulary, which maps each word to an ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETIONS_MODEL = \"text-davinci-003\"\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sorry, I don't know.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Answer the question only when the answer is in the training data, and if you're unsure of the answer, say \"Sorry, I don't know\".\n",
    "\n",
    "Context:\n",
    "You can pay online with Bancontact (card and app), credit card (Visa or MasterCard), PayPal, Apple Pay, Coolblue gift cards, or a bank transfer.\n",
    "In the store, you can easily pay with Bancontact, credit card, cash, Apple Pay, Coolblue gift cards, Consumption Passes, and EcoCheques.\n",
    "\n",
    "Q: Where can I find my invoice ?\n",
    "A:\"\"\"\n",
    "\n",
    "\n",
    "openai.Completion.create(\n",
    "    prompt=prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=300,\n",
    "    model=COMPLETIONS_MODEL\n",
    ")[\"choices\"][0][\"text\"].strip(\" \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
